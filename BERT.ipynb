{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8QxELwb+Ick7ugV/FrUGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanakashgouli/Research/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before transformers, LSTM (Long Short-Term Memory) were used to solve Natural Machine Translation problem.\n",
        "\n",
        "But they are slow to train, words are passed and generated sequentially, it takes a long time to learn and it does not capture the real meaning of the words.\n",
        "\n",
        "Bert stands for Bidirectional Encoder Representations from Transformers.\n",
        "\n",
        "It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=10mfXoYBty6IjzCR5OsILZ7F_iVXO0wxP)\n",
        "\n",
        "Transformers consist of encoder and decoder. Encoder takes the words and generates embeddings which are vectors that encapsulate the meaning of the word. Similar words have closer numbers in their vectors. Decoder takes the embeddings from encoder and previous words generated from the sentence that is to be translated. Encoder learns what language, grammar and context are, and decoder learns how language 1 relates to language 2.\n",
        "\n",
        "Since both encoder and decoder have an underlying understanding of language, we can pick apart this architecture and build a system that understands language.\n",
        "\n",
        "If we stack the decoders, we get the GPT transformer.\n",
        "\n",
        "If we stack the encoders, we get BERT.\n",
        "\n",
        "![BERT](https://drive.google.com/uc?export=view&id=1GFRuHRtlY2dU0AwYImUQhJ0msLy2JwWK)\n",
        "\n",
        "There\n",
        "\n"
      ],
      "metadata": {
        "id": "E12hh9TGEx-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources"
      ],
      "metadata": {
        "id": "f2CUKqVOGuzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: https://www.youtube.com/watch?v=xI0HHN5XKDo&ab_channel=CodeEmporium\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/bert\n",
        "\n",
        "https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c\n",
        "\n",
        "Paper: https://arxiv.org/abs/1810.04805"
      ],
      "metadata": {
        "id": "eWPyjI4kG2kI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UG9DV-H08wNy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}